"""
è´å¶æ–¯ ELO + é«˜çº§è’™ç‰¹å¡æ´›ç²‰ä¸æŠ•ç¥¨é€†å‘æ¨ç®—ç³»ç»Ÿ v2.0
==================================================
ç®—æ³•ä¼˜åŒ–ç‰¹æ€§:
- è´å¶æ–¯ ELO æ›´æ–°: å…ˆéªŒä¸ç¡®å®šæ€§å»ºæ¨¡ + åéªŒæ”¶ç¼©
- åˆ†å±‚è’™ç‰¹å¡æ´› (Stratified MC): æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·æé«˜æ”¶æ•›æ•ˆç‡
- æ—¶é—´è¡°å‡è®°å¿†: å†å²è¡¨ç°æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
- å¤šå› ç´ èåˆ: æ•´åˆæ’åã€å¾—åˆ†ã€ç”Ÿå­˜è½®æ¬¡çš„ç»¼åˆè¯„ä¼°
- Glicko-2 é£æ ¼çš„è¯„åˆ†ä¸ç¡®å®šæ€§è¿½è¸ª
- Bootstrap é‡é‡‡æ ·ç½®ä¿¡åŒºé—´

å¯è§†åŒ–ç‰¹æ€§ (Matplotlib):
- ELO æ¼”åŒ–è½¨è¿¹å›¾
- ç²‰ä¸æŠ•ç¥¨åˆ†å¸ƒçƒ­åŠ›å›¾
- è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ”¶æ•›è¯Šæ–­
- èµ›å­£å¯¹æ¯”é›·è¾¾å›¾
- é€‰æ‰‹æ’åæ¡å½¢å›¾
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, Optional, List, Any
import os
from functools import lru_cache
from dataclasses import dataclass, field
import warnings

# å°è¯•å¯¼å…¥ Numba
try:
    from numba import njit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    def njit(*args, **kwargs):
        def decorator(func):
            return func
        return decorator if args and callable(args[0]) else decorator
    prange = range

# å¯¼å…¥ Matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.ticker as mticker

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-whitegrid')


# ============== æ•°æ®ç»“æ„ ==============

@dataclass
class ContestantState:
    """é€‰æ‰‹çŠ¶æ€è¿½è¸ª (Glicko-2 é£æ ¼)"""
    elo: float = 1500.0           # å½“å‰è¯„åˆ†
    rd: float = 350.0             # è¯„åˆ†åå·® (Rating Deviation)
    volatility: float = 0.06      # æ³¢åŠ¨æ€§
    history: List[float] = field(default_factory=list)  # å†å²è¯„åˆ†
    weeks_active: int = 0         # æ´»è·ƒå‘¨æ•°
    total_score: float = 0.0      # ç´¯è®¡å¾—åˆ†


# ============== Numba åŠ é€Ÿæ ¸å¿ƒå‡½æ•° ==============

@njit(cache=True, fastmath=True)
def _latin_hypercube_sample(n_samples: int, n_dims: int) -> np.ndarray:
    """
    æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ · - æ¯”éšæœºé‡‡æ ·æ›´å‡åŒ€è¦†ç›–å‚æ•°ç©ºé—´
    æ”¶æ•›é€Ÿåº¦æå‡çº¦ sqrt(n) å€
    """
    result = np.empty((n_samples, n_dims))
    for dim in range(n_dims):
        # åœ¨æ¯ä¸ªç»´åº¦ä¸Šåˆ†å±‚é‡‡æ ·
        perm = np.random.permutation(n_samples)
        for i in range(n_samples):
            result[i, dim] = (perm[i] + np.random.random()) / n_samples
    return result

@njit(cache=True, fastmath=True)
def _get_ranks(values: np.ndarray) -> np.ndarray:
    """
    Numba ä¼˜åŒ–çš„æ’åè®¡ç®— (ç­‰åŒäº scipy.stats.rankdata, method='ordinal')
    è¿”å› 0 åˆ° n-1 çš„æ’åï¼Œå€¼è¶Šå¤§æ’åè¶Šé«˜
    """
    # argsort ä¸¤æ¬¡å¯å¾—æ’åç´¢å¼•
    return np.argsort(np.argsort(values))

# [æ›¿æ¢æ“ä½œ] æ›¿æ¢åŸæœ‰çš„ def _stratified_monte_carlo(...) æ•´ä¸ªå‡½æ•°å—

@njit(cache=False, fastmath=True)
def _stratified_monte_carlo(
    j_pct: np.ndarray, 
    f_pct: np.ndarray, 
    n_sim: int,
    noise_std: float,
    judge_weight: float,
    use_ranking_rule: bool = False # <--- æ³¨æ„è¿™é‡Œæ–°å¢äº†å‚æ•°
) -> Tuple[np.ndarray, np.ndarray]:
    """
    åˆ†å±‚è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ - æ”¯æŒç™¾åˆ†æ¯”æ³•ä¸æ’åæ³• (Season 1-2 vs Season 3+)
    """
    n = len(j_pct)
    death_counts = np.zeros(n, dtype=np.float64)
    score_sums = np.zeros(n, dtype=np.float64)
    
    # é¢„è®¡ç®—è£åˆ¤æ’å (ä»…åœ¨æ’åæ¨¡å¼ä¸‹ä½¿ç”¨)
    # æ³¨æ„ï¼šåˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé«˜ (0 ä¸ºæœ€ä½åˆ†)
    if use_ranking_rule:
        j_ranks = _get_ranks(j_pct).astype(np.float64)
    else:
        j_ranks = np.zeros(n) # å ä½
    
    for sim in range(n_sim):
        # 1. ç”ŸæˆæŠ—æ–¹å·®å™ªå£° (Antithetic Variates)
        noise = np.empty(n)
        anti_noise = np.empty(n)
        for i in range(n):
            z = np.random.randn()
            noise[i] = 1.0 + z * noise_std
            anti_noise[i] = 1.0 - z * noise_std
        
        # --- æ­£å‘æ¨¡æ‹Ÿ ---
        sim_f = f_pct * noise
        # å½’ä¸€åŒ–
        s_sum = 0.0
        for i in range(n): s_sum += sim_f[i]
        if s_sum > 1e-9:
            for i in range(n): sim_f[i] /= s_sum
            
        # [æ ¸å¿ƒé€»è¾‘åˆ†æ”¯]
        if use_ranking_rule:
            # æ’åæ³•: Score = Rank_J + Rank_F
            # Tie-Breaker: è‹¥æ€»æ’åç›¸åŒï¼Œç²‰ä¸æ’åä½è€…æ·˜æ±°ã€‚
            # æ•°å­¦å®ç°: Total = Rank_J + Rank_F + (Rank_F * 0.01)
            f_ranks = _get_ranks(sim_f).astype(np.float64)
            current_scores = j_ranks + f_ranks + (f_ranks * 0.01)
        else:
            # ç™¾åˆ†æ¯”æ³•
            current_scores = judge_weight * j_pct + (1 - judge_weight) * sim_f

        # è®°å½•æ·˜æ±°è€… (æœ€ä½åˆ†è€…)ä¸æ€»åˆ†
        min_val = current_scores[0]
        min_idx = 0
        for i in range(n):
            score_sums[i] += current_scores[i]
            if current_scores[i] < min_val:
                min_val = current_scores[i]
                min_idx = i
        death_counts[min_idx] += 0.5
        
        # --- åå‘æ¨¡æ‹Ÿ (Antithetic) ---
        sim_f_anti = f_pct * anti_noise
        s_sum = 0.0
        for i in range(n): s_sum += sim_f_anti[i]
        if s_sum > 1e-9:
            for i in range(n): sim_f_anti[i] /= s_sum
            
        if use_ranking_rule:
            f_ranks_anti = _get_ranks(sim_f_anti).astype(np.float64)
            current_scores = j_ranks + f_ranks_anti + (f_ranks_anti * 0.01)
        else:
            current_scores = judge_weight * j_pct + (1 - judge_weight) * sim_f_anti

        min_val = current_scores[0]
        min_idx = 0
        for i in range(n):
            score_sums[i] += current_scores[i]
            if current_scores[i] < min_val:
                min_val = current_scores[i]
                min_idx = i
        death_counts[min_idx] += 0.5
    
    return death_counts, score_sums / (n_sim * 2)

@njit(cache=True, fastmath=True)
def _bayesian_elo_update(
    elos: np.ndarray,
    rds: np.ndarray,
    j_pct: np.ndarray,
    f_pct: np.ndarray,
    loser_idx: int,
    base_k: float,
    rd_decay: float,
    use_ranking_rule: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    """
    è´å¶æ–¯ ELO æ›´æ–° - è€ƒè™‘è¯„åˆ†ä¸ç¡®å®šæ€§
    
    æ ¸å¿ƒæ€æƒ³:
    - RD å¤§çš„é€‰æ‰‹æ›´æ–°å¹…åº¦å¤§ (ä¿¡æ¯é‡å°‘ï¼Œéœ€è¦æ›´å¤šå­¦ä¹ )
    - RD éšæ—¶é—´è¡°å‡ï¼Œæ´»è·ƒæ¯”èµ›åè¿›ä¸€æ­¥é™ä½
    - ä½¿ç”¨ Glicko é£æ ¼çš„ g å‡½æ•°è°ƒæ•´æœŸæœ›å€¼
    """
    n = len(elos)
    new_elos = elos.copy()
    new_rds = rds.copy()
    
    if use_ranking_rule:
        # æ’åæ³•ï¼šåˆ†æ•°è¶Šé«˜è¶Šå¥½ (0..N-1)
        # argsort(argsort(x)) å¾—åˆ°çš„æ˜¯å…ƒç´ çš„æ’å (0æ˜¯æœ€å°/æœ€å·®ï¼ŒN-1æ˜¯æœ€å¤§/æœ€å¥½)
        # è¿™ä¸ ELO çš„ Z-score é€»è¾‘ï¼ˆåˆ†é«˜è€…ç”Ÿå­˜ï¼‰å®Œç¾å¥‘åˆ
        rank_j = np.argsort(np.argsort(j_pct))
        rank_f = np.argsort(np.argsort(f_pct))
        # ç®€å•ç›¸åŠ å³å¯ï¼ŒZ-score ä¼šè‡ªåŠ¨å¤„ç†é‡çº²
        total_scores = rank_j.astype(np.float64) + rank_f.astype(np.float64)
    else:
        # ç™¾åˆ†æ¯”æ³•ï¼šä¿æŒåŸæœ‰é€»è¾‘
        total_scores = 0.5 * j_pct + 0.5 * f_pct
    # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---
    
    avg_total = 0.0
    for i in range(n):
        avg_total += total_scores[i]
    avg_total /= n
    
    # è®¡ç®—æ ‡å‡†å·®
    variance = 0.0
    for i in range(n):
        diff = total_scores[i] - avg_total
        variance += diff * diff
    std_total = np.sqrt(variance / n) if n > 1 else 1.0
    std_total = max(std_total, 0.01)
    
    # Glicko g å‡½æ•°: g(RD) = 1 / sqrt(1 + 3*q^2*RD^2/pi^2)
    q = 0.0057565  # ln(10)/400
    pi_sq = 9.8696044
    
    for i in range(n):
        actual_survival = 0.0 if i == loser_idx else 1.0
        
        # z-score æ ‡å‡†åŒ–
        z_score = (total_scores[i] - avg_total) / std_total
        
        # Logistic æœŸæœ›ç”Ÿå­˜ç‡
        x = z_score * 2.5
        if x > 20:
            expected_survival = 1.0
        elif x < -20:
            expected_survival = 0.0
        else:
            expected_survival = 1.0 / (1.0 + np.exp(-x))
        
        # g å‡½æ•°è°ƒæ•´ (è€ƒè™‘ä¸ç¡®å®šæ€§)
        g_rd = 1.0 / np.sqrt(1.0 + 3.0 * q * q * rds[i] * rds[i] / pi_sq)
        
        # è‡ªé€‚åº” K å› å­ (RD è¶Šå¤§ï¼Œæ›´æ–°è¶Šå¤§)
        rd_factor = rds[i] / 350.0  # å½’ä¸€åŒ–
        surprise = abs(actual_survival - expected_survival)
        adaptive_k = base_k * rd_factor * (1.0 + 0.5 * surprise)
        
        # ELO æ›´æ–°
        delta = adaptive_k * g_rd * (actual_survival - expected_survival)
        new_elos[i] += delta
        
        # RD æ›´æ–° (æ¯”èµ›åé™ä½ä¸ç¡®å®šæ€§)
        new_rds[i] = max(30.0, rds[i] * rd_decay - abs(delta) * 0.1)
    
    return new_elos, new_rds


@njit(cache=True, fastmath=True)
def _compute_kl_divergence(p: np.ndarray, q: np.ndarray) -> float:
    """è®¡ç®— KL æ•£åº¦ D_KL(P || Q)"""
    kl = 0.0
    for i in range(len(p)):
        if p[i] > 1e-9 and q[i] > 1e-9:
            kl += p[i] * np.log(p[i] / q[i])
    return kl


@njit(cache=True, fastmath=True)
def _softmax_with_temperature(elos: np.ndarray, temperature: float) -> np.ndarray:
    """å¸¦æ¸©åº¦çš„ Softmax"""
    scaled = elos / temperature
    max_val = scaled[0]
    for i in range(1, len(scaled)):
        if scaled[i] > max_val:
            max_val = scaled[i]
    
    exp_vals = np.empty(len(scaled))
    sum_exp = 0.0
    for i in range(len(scaled)):
        exp_vals[i] = np.exp(scaled[i] - max_val)
        sum_exp += exp_vals[i]
    
    for i in range(len(scaled)):
        exp_vals[i] /= sum_exp
    
    return exp_vals


@njit(cache=True, fastmath=True)
def _compute_entropy(prob: np.ndarray) -> float:
    """è®¡ç®—ä¿¡æ¯ç†µ"""
    entropy = 0.0
    log2_e = 1.4426950408889634
    for p in prob:
        if p > 1e-9:
            entropy -= p * np.log(p) * log2_e
    return entropy


# ============== çº¯ NumPy å›é€€å‡½æ•° ==============

def _stratified_monte_carlo_numpy(
    j_pct: np.ndarray, 
    f_pct: np.ndarray, 
    n_sim: int,
    noise_std: float,
    judge_weight: float,
    use_ranking_rule: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    """NumPy å®ç°çš„åˆ†å±‚è’™ç‰¹å¡æ´›"""
    n = len(j_pct)
    
    # ç”Ÿæˆæ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·
    noise = np.random.normal(1.0, noise_std, (n_sim, n))
    
    # æ¨¡æ‹Ÿç²‰ä¸æŠ•ç¥¨
    sim_f = f_pct * noise
    sim_f = sim_f / (sim_f.sum(axis=1, keepdims=True) + 1e-9)
    
    # åŠ æƒæ€»åˆ†
    total_scores = judge_weight * j_pct + (1 - judge_weight) * sim_f
    
    # æ‰¾æ¯æ¬¡æ¨¡æ‹Ÿçš„æœ€ä½åˆ†
    loser_indices = np.argmin(total_scores, axis=1)
    death_counts = np.bincount(loser_indices, minlength=n).astype(np.float64)
    
    avg_scores = total_scores.mean(axis=0)
    
    return death_counts, avg_scores


# ============== ä¸»ç±»å®ç° ==============

class BayesianEloEstimator:
    """
    è´å¶æ–¯ ELO + é«˜çº§è’™ç‰¹å¡æ´›ç²‰ä¸æŠ•ç¥¨é€†å‘æ¨ç®—å™¨ v2.0
    """
    
    DEFAULT_ELO = 1500.0
    DEFAULT_RD = 350.0
    MIN_ELO = 800.0
    MAX_ELO = 2200.0
    
    def __init__(
        self,
        base_k_factor: float = 48.0,
        temperature: float = 150.0,
        n_simulations: int = 3000,
        noise_std: float = 0.15,
        judge_weight: float = 0.4,
        rd_decay: float = 0.95,
        use_adaptive_params: bool = True,
        memory_decay: float = 0.92
    ):
        """
        å‚æ•°:
            base_k_factor: åŸºç¡€å­¦ä¹ ç‡
            temperature: Softmax æ¸©åº¦
            n_simulations: è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°
            noise_std: ç²‰ä¸æŠ•ç¥¨æ³¢åŠ¨æ ‡å‡†å·®
            judge_weight: è¯„å§”åˆ†æ•°æƒé‡ (0-1)
            rd_decay: è¯„åˆ†åå·®è¡°å‡ç‡
            use_adaptive_params: æ˜¯å¦è‡ªé€‚åº”è°ƒæ•´å‚æ•°
            memory_decay: å†å²è®°å¿†è¡°å‡å› å­
        """
        self.base_k_factor = base_k_factor
        self.base_temperature = temperature
        self.n_simulations = n_simulations
        self.noise_std = noise_std
        self.judge_weight = judge_weight
        self.rd_decay = rd_decay
        self.use_adaptive_params = use_adaptive_params
        self.memory_decay = memory_decay
        
        # é€‰æ‰‹çŠ¶æ€å­˜å‚¨
        self.contestants: Dict[str, ContestantState] = {}
        
        # å†å²æ•°æ®è¿½è¸ª (ç”¨äºå¯è§†åŒ–)
        self.elo_history: List[Dict] = []
        self.mc_convergence: List[Dict] = []
        self.weekly_distributions: List[Dict] = []
        
        # é€‰æ‹©è®¡ç®—åç«¯
        self._mc_func = (_stratified_monte_carlo 
                         if NUMBA_AVAILABLE 
                         else _stratified_monte_carlo_numpy)
        
        self._total_simulations = 0
    
    def get_contestant(self, name: str) -> ContestantState:
        """è·å–æˆ–åˆ›å»ºé€‰æ‰‹çŠ¶æ€"""
        if name not in self.contestants:
            self.contestants[name] = ContestantState()
        return self.contestants[name]
    
    def get_elos_array(self, names: np.ndarray) -> np.ndarray:
        """æ‰¹é‡è·å– ELO æ•°ç»„"""
        return np.array([self.get_contestant(n).elo for n in names], dtype=np.float64)
    
    def get_rds_array(self, names: np.ndarray) -> np.ndarray:
        """æ‰¹é‡è·å– RD æ•°ç»„"""
        return np.array([self.get_contestant(n).rd for n in names], dtype=np.float64)
    
    def _get_adaptive_params(self, week: int, total_weeks: int, n_contestants: int) -> Tuple[float, float, float]:
        """
        è‡ªé€‚åº”å‚æ•°è°ƒæ•´
        è¿”å›: (temperature, noise_std, judge_weight)
        """
        if not self.use_adaptive_params or total_weeks <= 1:
            return self.base_temperature, self.noise_std, self.judge_weight
        
        progress = week / total_weeks
        
        # æ¸©åº¦: åæœŸé™ä½ (åˆ†å¸ƒæ›´é›†ä¸­)
        temperature = self.base_temperature * (1.0 - 0.3 * progress)
        
        # å™ªå£°: äººå°‘æ—¶ä¿æŒè¾ƒé«˜ (æŠ•ç¥¨æ›´ä¸ç¡®å®š)
        contestant_factor = min(1.0, n_contestants / 10.0)
        noise_std = self.noise_std * (0.8 + 0.4 * contestant_factor)
        
        # è¯„å§”æƒé‡: åæœŸç•¥å¢ (ä¸“ä¸šæ€§æ›´é‡è¦)
        judge_weight = self.judge_weight + 0.15 * progress
        judge_weight = min(0.6, judge_weight)
        
        return temperature, noise_std, judge_weight
    
    def calculate_metrics(
        self, 
        names: np.ndarray,
        j_pct: np.ndarray, 
        f_pct: np.ndarray, 
        loser_name: Optional[str],
        season: int,   # ç¡®ä¿è¿™ä¸¤ä¸ªå‚æ•°å­˜åœ¨
        week: int
    ) -> Dict[str, Any]:
        """
        è®¡ç®—å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ (è‡ªåŠ¨åˆ¤æ–­ Season 1-2 ä½¿ç”¨æ’ååˆ¶)
        """
        n = len(names)
        
        if not loser_name or loser_name not in names:
            return {
                'consistency': np.nan, 'certainty': np.nan,
                'ci_lower': np.nan, 'ci_upper': np.nan,
                'f_ci_lower': np.full(n, np.nan), 'f_ci_upper': np.full(n, np.nan),
                'kl_divergence': np.nan, 'effective_sample_size': float(self.n_simulations),
                'rule_used': 'none'
            }
        
        # è¾“å…¥æ ¡éªŒä¸è§„èŒƒåŒ–: ç¡®ä¿æ˜¯éè´Ÿä¸”å’Œä¸º1çš„æ¯”ä¾‹åˆ†å¸ƒ
        j_pct = j_pct.astype(np.float64)
        f_pct = f_pct.astype(np.float64)
        # å¤„ç†å¼‚å¸¸å€¼
        if not np.isfinite(j_pct).all() or (j_pct < 0).any() or j_pct.sum() <= 0:
            # é€€åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
            j_pct = np.ones(len(names), dtype=np.float64) / len(names)
        else:
            j_pct = j_pct / j_pct.sum()
        if not np.isfinite(f_pct).all() or (f_pct < 0).any() or f_pct.sum() <= 0:
            f_pct = np.ones(len(names), dtype=np.float64) / len(names)
        else:
            f_pct = f_pct / f_pct.sum()
        
        use_ranking_rule = False#(season <= 2 or season > 27)

        # [ä¿®æ”¹] è°ƒç”¨è’™ç‰¹å¡æ´›å‡½æ•°ï¼Œä¼ å…¥ use_ranking_rule
        death_counts, avg_scores = self._mc_func(
            j_pct,
            f_pct,
            self.n_simulations,
            self.noise_std,
            self.judge_weight,
            use_ranking_rule  # <--- æ–°å¢å‚æ•°
        )

        # æ£€æŸ¥è¿”å›å€¼çš„åˆç†æ€§, è‹¥å‘ç°éæœ‰é™æˆ–å’Œå¼‚å¸¸åˆ™å›é€€åˆ° NumPy å®ç°
        if (not np.isfinite(avg_scores).all()) or np.any(avg_scores < 0) or abs(np.sum(avg_scores)) <= 1e-12:
            try:
                death_counts, avg_scores = _stratified_monte_carlo_numpy(
                    j_pct, f_pct, self.n_simulations, self.noise_std, self.judge_weight
                )
            except Exception:
                # æœ€åä¿åº•: è®¾å‡åŒ€åˆ†å¸ƒ
                death_counts = np.ones(len(names), dtype=np.float64) * (self.n_simulations / len(names))
                avg_scores = np.ones(len(names), dtype=np.float64) / len(names)
        self._total_simulations += self.n_simulations
        
        prob_death = death_counts / self.n_simulations
        prob_death += 1e-9
        prob_death= prob_death / np.sum(prob_death)
        
        loser_idx = np.where(names == loser_name)[0][0]
        loser_prob = prob_death[loser_idx]
        max_prob = np.max(prob_death)
        
        # 1. ä¸€è‡´æ€§: ä½¿ç”¨æ’ååˆ†ä½æ•° + æ¦‚ç‡æ¯”ä¾‹çš„ç»¼åˆæŒ‡æ ‡
        # æ’åéƒ¨åˆ†: è¢«æ·˜æ±°è€…åœ¨æ‰€æœ‰äººä¸­çš„ç´¯ç§¯åˆ†å¸ƒåˆ†ä½
        sorted_probs = np.sort(prob_death)[::-1]  # é™åº
        rank_idx = np.searchsorted(-sorted_probs, -loser_prob)  # æ‰¾åˆ°æ’å
        rank_percentile = 1.0 - rank_idx / max(n - 1, 1)
        # æ¦‚ç‡æ¯”ä¾‹éƒ¨åˆ†
        prob_ratio = loser_prob / max_prob if max_prob > 0 else 0.0
        # ç»¼åˆæŒ‡æ ‡ (0.5*æ’å + 0.5*æ¦‚ç‡æ¯”ä¾‹)
        consistency = 0.5 * rank_percentile + 0.5 * prob_ratio
        
        # 2. ç¡®å®šæ€§: ä½¿ç”¨æœ‰æ•ˆé€‰æ‰‹æ•°çš„å½’ä¸€åŒ–ç‰ˆæœ¬
        # ESS = 1/sum(p^2)ï¼Œå½’ä¸€åŒ–åˆ° [0,1]
        ess_raw = 1.0 / np.sum(prob_death ** 2)
        certainty = 1.0 - (ess_raw - 1) / max(n - 1, 1)  # ESS=1æ—¶ç¡®å®šæ€§=1, ESS=næ—¶ç¡®å®šæ€§=0
        
        # 3. Bootstrap ç½®ä¿¡åŒºé—´
        n_boot = 500
        boot_probs = np.zeros(n_boot)
        for b in range(n_boot):
            boot_counts = np.random.multinomial(self.n_simulations, prob_death)
            boot_probs[b] = boot_counts[loser_idx] / self.n_simulations
        ci_lower = np.percentile(boot_probs, 2.5)
        ci_upper = np.percentile(boot_probs, 97.5)
        
        # [æ–°å¢] 3.5 ä¼°è®¡å¾—ç¥¨ç‡ (Fan Vote) çš„ 95% ç½®ä¿¡åŒºé—´
        # åŸºäºæ¨¡å‹è®¾å®šçš„ noise_stdï¼Œé€šè¿‡è’™ç‰¹å¡æ´›é‡‡æ ·ç”Ÿæˆæ‰€æœ‰é€‰æ‰‹çš„å¾—ç¥¨ç‡åˆ†å¸ƒ
        # è¿™é‡Œçš„ 2000 æ¬¡é‡‡æ ·è¶³ä»¥è·å¾—æå…¶ç²¾ç¡®çš„ç½®ä¿¡åŒºé—´åˆ†ä½æ•°
        f_samples = f_pct * np.random.normal(1.0, self.noise_std, (2000, n))
        f_samples /= f_samples.sum(axis=1, keepdims=True) # å½’ä¸€åŒ–å¤„ç†
        
        # è®¡ç®—æ‰€æœ‰é€‰æ‰‹å¾—ç¥¨ç‡çš„ 2.5% å’Œ 97.5% åˆ†ä½æ•° (axis=0 è¡¨ç¤ºå¯¹æ¯ä¸€åˆ—å³æ¯ä¸ªé€‰æ‰‹è®¡ç®—)
        f_ci_low_array = np.percentile(f_samples, 2.5, axis=0)
        f_ci_high_array = np.percentile(f_samples, 97.5, axis=0)
        
        # 4. KL æ•£åº¦ (ä¸å‡åŒ€åˆ†å¸ƒçš„è·ç¦»)ï¼Œä½¿ç”¨å¯¹ç§°åŒ–çš„JSæ•£åº¦
        uniform = np.ones(n) / n
        # ä½¿ç”¨ JS æ•£åº¦ (å¯¹ç§°ä¸”æœ‰ç•Œ [0, 1])
        m = 0.5 * (prob_death + uniform)
        js_div = 0.5 * np.sum(prob_death * np.log(prob_death / m + 1e-10)) + \
                 0.5 * np.sum(uniform * np.log(uniform / m + 1e-10))
        # JSæ•£åº¦çš„æœ€å¤§å€¼æ˜¯log(2)â‰ˆ 0.693
        kl_normalized = js_div / np.log(2)
        
        # 5. æœ‰æ•ˆæ ·æœ¬é‡ (ESS)ï¼Œå½’ä¸€åŒ–
        ess = 1.0 / np.sum(prob_death ** 2)
        ess_normalized = ess / n  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        # ä¿å­˜åˆ†å¸ƒç”¨äºå¯è§†åŒ–
        self.weekly_distributions.append({
            'season': season, 'week': week,
            'names': list(names),
            'prob_death': list(prob_death),
            'avg_scores': list(avg_scores),
            'loser': loser_name
        })
        
        return {
            'consistency': consistency,
            'certainty': certainty,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'f_ci_lower': f_ci_low_array,   # [æ–°å¢] å­˜å‚¨å…¨å‘˜ç²‰ä¸ç¥¨ä¸‹ç•Œæ•°ç»„
            'f_ci_upper': f_ci_high_array,
            'kl_divergence': kl_normalized,
            'effective_sample_size': ess_normalized,
            'rule_used': 'rank' if use_ranking_rule else 'percent' # <--- æ–°å¢è®°å½•
        }
    
    def _update_contestants(
        self,
        names: np.ndarray,
        j_pct: np.ndarray,
        f_pct: np.ndarray,
        loser_name: str,
        season: int
    ) -> None:
        """æ›´æ–°æ‰€æœ‰é€‰æ‰‹çŠ¶æ€"""
        loser_idx = np.where(names == loser_name)[0][0]
        
        current_elos = self.get_elos_array(names)
        current_rds = self.get_rds_array(names)
        
        use_ranking_rule = False#(season <= 2 or season > 27)
        
        if NUMBA_AVAILABLE:
            new_elos, new_rds = _bayesian_elo_update(
                current_elos, current_rds, j_pct, f_pct,
                loser_idx, self.base_k_factor, self.rd_decay
            )
        else:
            # NumPy å›é€€
            if use_ranking_rule:
                rank_j = np.argsort(np.argsort(j_pct))
                rank_f = np.argsort(np.argsort(f_pct))
                total_scores = rank_j + rank_f
            else:
                total_scores = 0.5 * j_pct + 0.5 * f_pct
            avg_total = np.mean(total_scores)
            std_total = max(np.std(total_scores), 0.01)
            
            z_scores = (total_scores - avg_total) / std_total
            expected_survival = 1.0 / (1.0 + np.exp(-z_scores * 2.5))
            
            actual_survival = np.ones(len(names))
            actual_survival[loser_idx] = 0.0
            
            rd_factor = current_rds / 350.0
            surprise = np.abs(actual_survival - expected_survival)
            adaptive_k = self.base_k_factor * rd_factor * (1.0 + 0.5 * surprise)
            
            new_elos = current_elos + adaptive_k * (actual_survival - expected_survival)
            new_rds = np.maximum(30.0, current_rds * self.rd_decay)
        
        # åº”ç”¨è¾¹ç•Œçº¦æŸå¹¶æ›´æ–°
        new_elos = np.clip(new_elos, self.MIN_ELO, self.MAX_ELO)
        
        for i, name in enumerate(names):
            contestant = self.get_contestant(name)
            contestant.elo = new_elos[i]
            contestant.rd = new_rds[i]
            contestant.history.append(new_elos[i])
            contestant.weeks_active += 1
            contestant.total_score += j_pct[i]
    
    def run_inference(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ‰§è¡Œå®Œæ•´çš„æ¨æ–­æµç¨‹"""
        df = df.copy()
        df['name'] = df['name'].str.strip()
        
        season_weeks = df.groupby('season')['week'].max().to_dict()
        results: List[Dict] = []
        
        seasons = sorted(df['season'].unique())
        
        for s in seasons:
            s_data = df[df['season'] == s]
            total_weeks = season_weeks[s]
            weeks = sorted(s_data['week'].unique())
            
            # èµ›å­£å¼€å§‹æ—¶è¡°å‡ RD (é•¿æœŸä¸æ´»è·ƒçš„é€‰æ‰‹)
            for name, contestant in self.contestants.items():
                contestant.rd = min(350.0, contestant.rd * 1.1)
            
            for w in weeks:
                # æ’é™¤å·²å‡ºå±€çš„é€‰æ‰‹ï¼Œä½†ä¿ç•™æœ¬å‘¨é€€å‡º(Withdrew)çš„é€‰æ‰‹ç”¨äºç»Ÿè®¡
                w_data = s_data[(s_data['week'] == w) & (s_data['status'] != 'Out')]
                if w_data.empty:
                    continue
                
                names = w_data['name'].values
                j_pct = w_data['judge_pct'].values.astype(np.float64)
                n_contestants = len(names)
                
                # è·å–è‡ªé€‚åº”å‚æ•°
                temperature, noise_std, judge_weight = self._get_adaptive_params(
                    w, total_weeks, n_contestants
                )
                self.noise_std = noise_std
                self.judge_weight = judge_weight
                
                # æ˜ å°„ ELO åˆ°ç²‰ä¸æŠ•ç¥¨
                current_elos = self.get_elos_array(names)
                current_rds = self.get_rds_array(names)
                
                if NUMBA_AVAILABLE:
                    f_pct = _softmax_with_temperature(current_elos, temperature)
                else:
                    scaled = current_elos / temperature
                    exp_scaled = np.exp(scaled - np.max(scaled))
                    f_pct = exp_scaled / exp_scaled.sum()
                
                # è¯†åˆ«è¢«æ·˜æ±°è€…å’Œé€€å‡ºè€…
                elim_mask = w_data['status'] == 'Eliminated'
                withdrew_mask = w_data['status'] == 'Withdrew'
                
                # è¢«æŠ•ç¥¨æ·˜æ±°çš„é€‰æ‰‹ï¼ˆç”¨äºELOæ›´æ–°ï¼‰
                actual_loser = w_data.loc[elim_mask, 'name'].values
                actual_loser = actual_loser[0] if len(actual_loser) > 0 else None
                
                # ä¸»åŠ¨é€€å‡ºçš„é€‰æ‰‹ï¼ˆä¸å‚ä¸ELOæ·˜æ±°è®¡ç®—ï¼Œä½†éœ€è¦è®°å½•ï¼‰
                withdrew_player = w_data.loc[withdrew_mask, 'name'].values
                withdrew_player = withdrew_player[0] if len(withdrew_player) > 0 else None
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self.calculate_metrics(names, j_pct, f_pct, actual_loser, s, w)
                
                # æ›´æ–° ELO
                if actual_loser:
                    self._update_contestants(names, j_pct, f_pct, actual_loser,season=s)
                
                # è®°å½•ç»“æœ
                for i, name in enumerate(names):
                    contestant = self.get_contestant(name)
                    
                    # åˆ¤æ–­é€‰æ‰‹çŠ¶æ€
                    is_withdrew = (name == withdrew_player)
                    is_eliminated = (name == actual_loser)
                    
                    # ä¿å­˜ ELO å†å²
                    self.elo_history.append({
                        'season': s, 'week': w, 'name': name,
                        'elo': contestant.elo, 'rd': contestant.rd
                    })
                    
                    results.append({
                        'season': s,
                        'week': w,
                        'name': name,
                        'judge_pct': j_pct[i],
                        'est_fan_pct': f_pct[i],
                        # [æ–°å¢] è®°å½•è¯¥é€‰æ‰‹ç²‰ä¸æŠ•ç¥¨ç‡çš„ 95% ç½®ä¿¡åŒºé—´
                        'fan_pct_ci_lower': metrics['f_ci_lower'][i],
                        'fan_pct_ci_upper': metrics['f_ci_upper'][i],
                        'elo_rating': contestant.elo,
                        'rating_deviation': contestant.rd,
                        'consistency_score': metrics['consistency'],
                        'certainty_score': metrics['certainty'],
                        'eli_ci_95_lower': metrics['ci_lower'],
                        'eli_ci_95_upper': metrics['ci_upper'],
                        'kl_divergence': metrics['kl_divergence'],
                        'effective_sample_size': metrics['effective_sample_size'],
                        'is_withdrew': is_withdrew,  # ä¸»åŠ¨é€€å‡ºæ ‡è®°
                        'is_eliminated': is_eliminated  # è¢«æ·˜æ±°æ ‡è®°
                    })
        
        return pd.DataFrame(results)
    
    def get_final_rankings(self) -> pd.DataFrame:
        """è·å–æœ€ç»ˆæ’å"""
        data = []
        for name, state in self.contestants.items():
            data.append({
                'name': name,
                'final_elo': state.elo,
                'rating_deviation': state.rd,
                'weeks_active': state.weeks_active,
                'avg_judge_score': state.total_score / max(1, state.weeks_active)
            })
        return pd.DataFrame(data).sort_values('final_elo', ascending=False).reset_index(drop=True)
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        elos = [c.elo for c in self.contestants.values()]
        return {
            'total_contestants': len(self.contestants),
            'total_simulations': self._total_simulations,
            'avg_elo': np.mean(elos),
            'elo_std': np.std(elos),
            'backend': 'Numba JIT' if NUMBA_AVAILABLE else 'NumPy'
        }




# ============== Matplotlib å¯è§†åŒ–æ¨¡å— ==============

class EloVisualizer:
    """
    é«˜çº§å¯è§†åŒ–ç±» - ä½¿ç”¨ Matplotlib ç”Ÿæˆé«˜è´¨é‡å›¾è¡¨
    """
    
    # ä¸“ä¸šé…è‰²æ–¹æ¡ˆ
    COLORS = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', 
              '#ffd92f', '#e5c494', '#b3b3b3', '#1f78b4', '#33a02c',
              '#fb9a99', '#e31a1c', '#ff7f00', '#cab2d6', '#6a3d9a']
    
    def __init__(self, estimator: 'BayesianEloEstimator', results_df: pd.DataFrame):
        self.estimator = estimator
        self.results = results_df
        self.output_dir = 'QFâ€˜s solution/Bayes_Elo/figures'
        os.makedirs(self.output_dir, exist_ok=True)
    
    def plot_elo_trajectories(self, top_n: int = 15, seasons: Optional[List[int]] = None):
        """
        ç»˜åˆ¶ Top N é€‰æ‰‹çš„ ELO æ¼”åŒ–è½¨è¿¹
        æ¯ä¸ªé€‰æ‰‹ä»ç¬¬1å‘¨å¼€å§‹ç»˜åˆ¶ï¼Œå±•ç¤ºå…¶åœ¨æ¯”èµ›ä¸­çš„æˆé•¿æ›²çº¿
        """
        # è·å– Top N é€‰æ‰‹
        rankings = self.estimator.get_final_rankings()
        top_names = rankings.head(top_n)['name'].tolist()
        
        # å‡†å¤‡æ•°æ®
        elo_df = pd.DataFrame(self.estimator.elo_history)
        if seasons:
            elo_df = elo_df[elo_df['season'].isin(seasons)]
        
        elo_df = elo_df[elo_df['name'].isin(top_names)]
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        for i, name in enumerate(top_names):
            name_data = elo_df[elo_df['name'] == name].copy()
            if name_data.empty:
                continue
            
            # æŒ‰èµ›å­£å’Œå‘¨æ’åº
            name_data = name_data.sort_values(['season', 'week'])
            
            # åˆ›å»ºè¿ç»­çš„å‘¨æ¬¡ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
            name_data['week_idx'] = range(1, len(name_data) + 1)
            
            color = self.COLORS[i % len(self.COLORS)]
            
            # è·å–é€‰æ‰‹å‚åŠ çš„èµ›å­£ä¿¡æ¯
            season_info = name_data['season'].iloc[0]
            
            # ELO æ›²çº¿
            ax.plot(name_data['week_idx'], name_data['elo'], 
                   color=color, linewidth=2, label=f'{name} (S{season_info})', 
                   marker='o', markersize=4)
            
            # RD ç½®ä¿¡å¸¦
            ax.fill_between(name_data['week_idx'],
                           name_data['elo'] - name_data['rd']/3,
                           name_data['elo'] + name_data['rd']/3,
                           color=color, alpha=0.12)
        
        # æ·»åŠ åŸºå‡†çº¿
        ax.axhline(y=1500, linestyle='--', color='gray', alpha=0.7, label='Initial ELO (1500)')
        
        ax.set_xlabel('Week in Competition', fontsize=12)
        ax.set_ylabel('ELO Rating', fontsize=12)
        ax.set_title(f'Top {top_n} Contestants ELO Rating Evolution', fontsize=16, fontweight='bold')
        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(left=0)
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, 'elo_trajectories.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ˆ ELO è½¨è¿¹å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_elo_by_season(self, season: int, top_n: int = 10):
        """
        ç»˜åˆ¶å•ä¸ªèµ›å­£å†…æ‰€æœ‰é€‰æ‰‹çš„ ELO æ¼”åŒ–è½¨è¿¹
        """
        # å‡†å¤‡æ•°æ®
        elo_df = pd.DataFrame(self.estimator.elo_history)
        season_data = elo_df[elo_df['season'] == season]
        
        if season_data.empty:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¬ {season} å­£æ•°æ®")
            return
        
        # è·å–è¯¥èµ›å­£æœ€ç»ˆ ELO æœ€é«˜çš„é€‰æ‰‹
        final_week = season_data['week'].max()
        final_elos = season_data[season_data['week'] == final_week].nlargest(top_n, 'elo')
        top_names = final_elos['name'].tolist()
        
        fig, ax = plt.subplots(figsize=(12, 7))
        
        for i, name in enumerate(top_names):
            name_data = season_data[season_data['name'] == name].sort_values('week')
            if name_data.empty:
                continue
            
            color = self.COLORS[i % len(self.COLORS)]
            
            ax.plot(name_data['week'], name_data['elo'], 
                   color=color, linewidth=2.5, label=name, 
                   marker='o', markersize=5)
            
            # RD ç½®ä¿¡å¸¦
            ax.fill_between(name_data['week'],
                           name_data['elo'] - name_data['rd']/3,
                           name_data['elo'] + name_data['rd']/3,
                           color=color, alpha=0.15)
        
        ax.axhline(y=1500, linestyle='--', color='gray', alpha=0.7, label='Initial ELO')
        ax.set_xlabel('Week', fontsize=12)
        ax.set_ylabel('ELO Rating', fontsize=12)
        ax.set_title(f'Season {season} ELO Rating Evolution (Top {top_n})', 
                    fontsize=14, fontweight='bold')
        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(left=0)
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, f'elo_season_{season}.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ˆ ç¬¬{season}å­£ ELO è½¨è¿¹å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_fan_vote_heatmap(self, season: int):
        """
        ç»˜åˆ¶å•èµ›å­£ç²‰ä¸æŠ•ç¥¨åˆ†å¸ƒçƒ­åŠ›å›¾
        """
        season_data = self.results[self.results['season'] == season]
        if season_data.empty:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¬ {season} å­£æ•°æ®")
            return
        
        # åˆ›å»ºé€è§†è¡¨
        pivot = season_data.pivot_table(
            values='est_fan_pct', 
            index='name', 
            columns='week',
            aggfunc='first'
        ).fillna(0)
        
        # æŒ‰æœ€åä¸€å‘¨çš„æŠ•ç¥¨æ’åº
        last_week = pivot.columns.max()
        pivot = pivot.sort_values(by=last_week, ascending=True)
        
        fig, ax = plt.subplots(figsize=(12, max(6, len(pivot) * 0.35)))
        
        # åˆ›å»ºçº¢ç»¿æ¸å˜è‰²å›¾
        cmap = LinearSegmentedColormap.from_list('RdYlGn', ['#d73027', '#fee08b', '#1a9850'])
        
        # è®¡ç®—åˆç†çš„è‰²åº¦èŒƒå›´ (ä½¿ç”¨ç™¾åˆ†ä½æ•°é¿å…æç«¯å€¼)
        valid_data = pivot.values[pivot.values > 0]
        if len(valid_data) > 0:
            vmin = np.percentile(valid_data, 5)  # 5%ç™¾åˆ†ä½
            vmax = np.percentile(valid_data, 95)  # 95%ç™¾åˆ†ä½
            # ç¡®ä¿èŒƒå›´åˆç†
            vmin = max(0, vmin - 0.02)
            vmax = min(1, vmax + 0.02)
        else:
            vmin, vmax = 0, 1
        
        im = ax.imshow(pivot.values, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)
        
        # è®¾ç½®åˆ»åº¦
        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels([f'W{w}' for w in pivot.columns], fontsize=9)
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index, fontsize=9)
        
        # ç§»é™¤ç½‘æ ¼çº¿æ•ˆæœ
        ax.set_xticks(np.arange(len(pivot.columns)+1)-0.5, minor=True)
        ax.set_yticks(np.arange(len(pivot.index)+1)-0.5, minor=True)
        ax.grid(False)
        ax.tick_params(which='minor', length=0)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾ - ä½¿ç”¨æ›´å¤§çš„å­—ä½“
        for i in range(len(pivot.index)):
            for j in range(len(pivot.columns)):
                val = pivot.values[i, j]
                if val > 0.005:  # åªæ˜¾ç¤ºå¤§äº0.5%çš„å€¼
                    # æ ¹æ®å€¼åˆ¤æ–­æ–‡æœ¬é¢œè‰²
                    relative_val = (val - vmin) / (vmax - vmin) if vmax > vmin else 0.5
                    text_color = 'white' if relative_val > 0.6 or relative_val < 0.3 else 'black'
                    ax.text(j, i, f'{val*100:.0f}', ha='center', va='center', 
                           fontsize=10, color=text_color, fontweight='bold')
        
        ax.set_xlabel('Week', fontsize=12)
        ax.set_ylabel('Contestant', fontsize=12)
        ax.set_title(f'Season {season} Fan Vote Distribution Heatmap', fontsize=14, fontweight='bold')
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label('Estimated Fan Vote %', fontsize=10)
        cbar.ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, f'heatmap_season_{season}.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ”¥ çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_model_diagnostics(self):
        """
        ç»˜åˆ¶æ¨¡å‹è¯Šæ–­å›¾ - ä¸€è‡´æ€§ã€ç¡®å®šæ€§åˆ†å¸ƒ
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # èšåˆåˆ°å‘¨çº§åˆ«ï¼Œå¹¶è¿‡æ»¤æ‰ NaN å€¼
        weekly = self.results.groupby(['season', 'week']).first().reset_index()
        weekly_valid = weekly.dropna(subset=['consistency_score', 'certainty_score', 'kl_divergence'])
        
        from scipy import stats
        
        # 1. ä¸€è‡´æ€§åˆ†å¸ƒ
        ax1 = axes[0, 0]
        consistency = weekly_valid['consistency_score']
        ax1.hist(consistency, bins=25, color='steelblue', edgecolor='white', alpha=0.7, density=True)
        if len(consistency) > 5:
            kde = stats.gaussian_kde(consistency, bw_method=0.3)
            x_range = np.linspace(0, 1, 100)
            ax1.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')
        ax1.axvline(consistency.mean(), color='darkred', linestyle='--', linewidth=2,
                   label=f'Mean: {consistency.mean():.3f}')
        ax1.axvline(consistency.median(), color='orange', linestyle=':', linewidth=2,
                   label=f'Median: {consistency.median():.3f}')
        ax1.set_xlabel('Consistency Score', fontsize=11)
        ax1.set_ylabel('Density', fontsize=11)
        ax1.set_title('Consistency Score Distribution\n(Higher = Better Prediction)', fontsize=12, fontweight='bold')
        ax1.set_xlim(0, 1.05)
        ax1.legend(fontsize=9)
        ax1.grid(alpha=0.3)
        
        # 2. ç¡®å®šæ€§åˆ†å¸ƒ (ç°åœ¨æ˜¯ 1-ESS/nï¼ŒèŒƒå›´æ›´åˆç†)
        ax2 = axes[0, 1]
        certainty = weekly_valid['certainty_score']
        ax2.hist(certainty, bins=25, color='forestgreen', edgecolor='white', alpha=0.7, density=True)
        if len(certainty) > 5 and certainty.std() > 0.01:
            kde = stats.gaussian_kde(certainty, bw_method=0.3)
            x_range = np.linspace(max(0, certainty.min()-0.05), min(1, certainty.max()+0.1), 100)
            ax2.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')
        ax2.axvline(certainty.mean(), color='darkred', linestyle='--', linewidth=2,
                   label=f'Mean: {certainty.mean():.3f}')
        ax2.set_xlabel('Certainty Score (1-ESS/n)', fontsize=11)
        ax2.set_ylabel('Density', fontsize=11)
        ax2.set_title('Certainty Score Distribution\n(Low = Uniform, High = Concentrated)', fontsize=12, fontweight='bold')
        ax2.legend(fontsize=9)
        ax2.grid(alpha=0.3)
        
        # 3. ä¸€è‡´æ€§ vs JSæ•£åº¦ æ•£ç‚¹å›¾ (æ›´æœ‰æ„ä¹‰çš„ç»„åˆ)
        ax3 = axes[1, 0]
        scatter = ax3.scatter(weekly_valid['consistency_score'], weekly_valid['kl_divergence'],
                             c=weekly_valid['season'], cmap='viridis', alpha=0.6, s=40, edgecolor='white', linewidth=0.5)
        ax3.set_xlabel('Consistency (Prediction Accuracy)', fontsize=11)
        ax3.set_ylabel('JS Divergence (Distribution Skewness)', fontsize=11)
        ax3.set_title('Consistency vs Distribution Skewness', fontsize=12, fontweight='bold')
        ax3.set_xlim(0, 1.05)
        cbar = plt.colorbar(scatter, ax=ax3)
        cbar.set_label('Season', fontsize=10)
        ax3.grid(alpha=0.3)
        
        # 4. ä¸€è‡´æ€§æŒ‰èµ›å­£è¶‹åŠ¿
        ax4 = axes[1, 1]
        season_stats = weekly_valid.groupby('season')['consistency_score'].agg(['mean', 'std']).reset_index()
        ax4.bar(season_stats['season'], season_stats['mean'], color='coral', alpha=0.7, edgecolor='white')
        ax4.errorbar(season_stats['season'], season_stats['mean'], yerr=season_stats['std'], 
                    fmt='none', color='darkred', capsize=3, alpha=0.7)
        ax4.set_xlabel('Season', fontsize=11)
        ax4.set_ylabel('Consistency Score', fontsize=11)
        ax4.set_title('Prediction Accuracy by Season', fontsize=12, fontweight='bold')
        ax4.set_ylim(0, 1.1)
        ax4.axhline(y=season_stats['mean'].mean(), linestyle='--', color='red', alpha=0.7,
                   label=f'Overall Mean: {season_stats["mean"].mean():.3f}')
        ax4.axhline(y=0.5, linestyle=':', color='gray', alpha=0.5, label='Random Guess (0.5)')
        ax4.legend(fontsize=9)
        ax4.grid(alpha=0.3)
        
        plt.suptitle('Model Diagnostics Dashboard', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        output_path = os.path.join(self.output_dir, 'model_diagnostics.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š è¯Šæ–­å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_season_comparison_radar(self, seasons: List[int] = None):
        """
        ç»˜åˆ¶èµ›å­£å¯¹æ¯”é›·è¾¾å›¾
        """
        if seasons is None:
            seasons = sorted(self.results['season'].unique())[-5:]  # æœ€è¿‘5ä¸ªèµ›å­£
        
        metrics = ['consistency_score', 'certainty_score', 'kl_divergence', 'effective_sample_size']
        metric_names = ['Consistency', 'Certainty', 'KL Divergence', 'Eff. Sample Size']
        
        # å…ˆæ”¶é›†æ‰€æœ‰èµ›å­£çš„åŸå§‹æ•°æ®ï¼Œç”¨äºè®¡ç®—å½’ä¸€åŒ–èŒƒå›´
        raw_values = {metric: [] for metric in metrics}
        for season in seasons:
            season_data = self.results[self.results['season'] == season]
            weekly = season_data.groupby('week').first()
            for metric in metrics:
                raw_values[metric].append(weekly[metric].mean())
        
        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„èŒƒå›´ç”¨äºå½’ä¸€åŒ– (ä½¿ç”¨ min-max å½’ä¸€åŒ–)
        metric_ranges = {}
        for metric in metrics:
            vals = raw_values[metric]
            min_v, max_v = min(vals), max(vals)
            # ç¨å¾®æ‰©å±•èŒƒå›´ï¼Œé¿å…è¾¹ç•Œå€¼
            range_v = max_v - min_v if max_v > min_v else 1
            metric_ranges[metric] = (min_v - 0.1 * range_v, max_v + 0.1 * range_v)
        
        # è®¡ç®—æ¯ä¸ªèµ›å­£çš„å½’ä¸€åŒ–æŒ‡æ ‡
        season_values = {}
        for season in seasons:
            season_data = self.results[self.results['season'] == season]
            weekly = season_data.groupby('week').first()
            
            values = []
            for metric in metrics:
                val = weekly[metric].mean()
                # Min-max å½’ä¸€åŒ–åˆ° 0.15-0.85 èŒƒå›´ï¼Œé¿å…æç«¯
                min_v, max_v = metric_ranges[metric]
                if max_v > min_v:
                    normalized = (val - min_v) / (max_v - min_v)
                    normalized = 0.15 + 0.70 * normalized  # æ˜ å°„åˆ° 0.15-0.85
                else:
                    normalized = 0.5
                values.append(normalized)
            season_values[season] = values
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))
        
        for i, season in enumerate(seasons):
            values = season_values[season] + season_values[season][:1]  # é—­åˆ
            color = self.COLORS[i % len(self.COLORS)]
            ax.plot(angles, values, 'o-', linewidth=2, label=f'Season {season}', color=color)
            ax.fill(angles, values, alpha=0.2, color=color)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metric_names, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_title('Season Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, 'season_radar.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ¯ é›·è¾¾å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_elimination_probability(self, season: int, week: int):
        """
        ç»˜åˆ¶ç‰¹å®šå‘¨çš„æ·˜æ±°æ¦‚ç‡åˆ†å¸ƒ (è’™ç‰¹å¡æ´›ç»“æœ)
        """
        # æŸ¥æ‰¾å¯¹åº”çš„æ¨¡æ‹Ÿç»“æœ
        dist_data = None
        for d in self.estimator.weekly_distributions:
            if d['season'] == season and d['week'] == week:
                dist_data = d
                break
        
        if dist_data is None:
            print(f"âš ï¸ æœªæ‰¾åˆ° S{season}W{week} çš„æ¨¡æ‹Ÿæ•°æ®")
            return
        
        names = dist_data['names']
        probs = np.array(dist_data['prob_death'])
        avg_scores = np.array(dist_data['avg_scores'])
        loser = dist_data['loser']
        
        # æŒ‰æ·˜æ±°æ¦‚ç‡æ’åº
        sorted_indices = np.argsort(probs)[::-1]
        names = [names[i] for i in sorted_indices]
        probs = probs[sorted_indices]
        avg_scores = avg_scores[sorted_indices]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾: æ·˜æ±°æ¦‚ç‡
        colors = ['crimson' if n == loser else 'steelblue' for n in names]
        bars1 = ax1.bar(range(len(names)), probs, color=colors, edgecolor='white', alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, prob in zip(bars1, probs):
            height = bar.get_height()
            if prob > 0.01:  # åªæ˜¾ç¤ºå¤§äº1%çš„æ ‡ç­¾
                ax1.annotate(f'{prob:.1%}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3), textcoords="offset points",
                           ha='center', va='bottom', fontsize=9)
        
        ax1.set_xticks(range(len(names)))
        ax1.set_xticklabels(names, rotation=45, ha='right', fontsize=10)
        ax1.set_ylabel('Elimination Probability', fontsize=12)
        ax1.set_title(f'Season {season} Week {week} Elimination Probability\n(Red = Actual Eliminated: {loser})',
                    fontsize=12, fontweight='bold')
        ax1.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))
        ax1.set_ylim(0, min(1.1, max(probs) * 1.2))
        
        # å³å›¾: æ¨¡æ‹Ÿå¹³å‡å¾—åˆ† (è½¬ä¸ºç™¾åˆ†æ¯”æ˜¾ç¤º)
        colors2 = ['crimson' if n == loser else 'forestgreen' for n in names]
        avg_scores_pct = avg_scores * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        bars2 = ax2.bar(range(len(names)), avg_scores_pct, color=colors2, edgecolor='white', alpha=0.8)
        
        for bar, score in zip(bars2, avg_scores_pct):
            ax2.annotate(f'{score:.1f}%',
                       xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                       xytext=(0, 3), textcoords="offset points",
                       ha='center', va='bottom', fontsize=9)
        
        ax2.set_xticks(range(len(names)))
        ax2.set_xticklabels(names, rotation=45, ha='right', fontsize=10)
        ax2.set_ylabel('Simulated Average Score (%)', fontsize=12)
        ax2.set_title(f'MC Simulated Average Scores\n(Lower score = Higher elimination risk)',
                    fontsize=12, fontweight='bold')
        # Yè½´æ˜¾ç¤ºæ•´æ•° (æ•°æ®å·²ç»æ˜¯ç™¾åˆ†æ¯”å½¢å¼)
        ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.0f'))
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, f'elim_prob_s{season}w{week}.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“‰ æ·˜æ±°æ¦‚ç‡å›¾å·²ä¿å­˜: {output_path}")
    
    def plot_final_rankings(self, top_n: int = 20):
        """
        ç»˜åˆ¶æœ€ç»ˆ ELO æ’åæ¡å½¢å›¾
        """
        rankings = self.estimator.get_final_rankings().head(top_n)
        
        fig, ax = plt.subplots(figsize=(14, max(8, top_n * 0.45)))
        
        # åˆ›å»ºæ¸å˜è‰² - ä»é‡‘è‰²åˆ°é“¶è‰²åˆ°é“œè‰²
        colors = []
        for i in range(len(rankings)):
            if i < 3:  # å‰ä¸‰åç”¨é‡‘é“¶é“œ
                colors.append(['#FFD700', '#C0C0C0', '#CD7F32'][i])
            else:
                # å…¶ä»–ç”¨æ¸å˜è“è‰²
                intensity = 0.8 - 0.5 * (i - 3) / max(len(rankings) - 4, 1)
                colors.append(plt.cm.Blues(intensity))
        
        y_pos = range(len(rankings))
        
        # ç»˜åˆ¶æ¡å½¢å›¾ï¼Œä½¿ç”¨ELOç›¸å¯¹äºåŸºå‡†çš„å·®å€¼
        baseline = 1500
        bar_values = rankings['final_elo'] - baseline
        
        bars = ax.barh(y_pos, bar_values, color=colors, edgecolor='darkgray', linewidth=0.5, height=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, elo, rd) in enumerate(zip(bars, rankings['final_elo'], rankings['rating_deviation'])):
            # åœ¨æ¡å½¢å›¾æœ«ç«¯æ˜¾ç¤º ELO åˆ†æ•°
            ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,
                   f'{elo:.0f}', va='center', fontsize=10, fontweight='bold')
        
        ax.set_yticks(y_pos)
        ax.set_yticklabels(rankings['name'], fontsize=11)
        ax.invert_yaxis()  # æœ€é«˜åˆ†åœ¨ä¸Š
        ax.set_xlabel('ELO Rating (relative to 1500)', fontsize=12)
        ax.set_title(f'Top {top_n} Contestants Final ELO Ranking', fontsize=14, fontweight='bold')
        
        # æ·»åŠ åŸºå‡†çº¿
        ax.axvline(x=0, linestyle='-', color='black', alpha=0.3, linewidth=1)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(axis='x', alpha=0.3, linestyle='--')
        ax.set_xlim(-50, max(bar_values) * 1.15)
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, 'final_rankings.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ† æ’åå›¾å·²ä¿å­˜: {output_path}")
    
    def plot_elo_distribution(self):
        """
        ç»˜åˆ¶æœ€ç»ˆ ELO åˆ†å¸ƒç›´æ–¹å›¾
        """
        elos = [c.elo for c in self.estimator.contestants.values()]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        ax.hist(elos, bins=40, color='steelblue', edgecolor='white', alpha=0.8)
        ax.axvline(np.mean(elos), color='red', linestyle='--', linewidth=2,
                  label=f'Mean: {np.mean(elos):.1f}')
        ax.axvline(np.median(elos), color='orange', linestyle='--', linewidth=2,
                  label=f'Median: {np.median(elos):.1f}')
        ax.axvline(1500, color='gray', linestyle=':', linewidth=2,
                  label='Initial: 1500')
        
        ax.set_xlabel('ELO Rating', fontsize=12)
        ax.set_ylabel('Number of Contestants', fontsize=12)
        ax.set_title('Final ELO Rating Distribution', fontsize=14, fontweight='bold')
        ax.legend()
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, 'elo_distribution.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ELOåˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")
    
    def plot_consistency_by_season(self):
        """
        ç»˜åˆ¶å„èµ›å­£ä¸€è‡´æ€§ç®±çº¿å›¾ï¼ˆæŒ‰æ—¶é—´æ®µåˆ†ç»„ï¼‰
        """
        weekly = self.results.groupby(['season', 'week']).first().reset_index()
        weekly_valid = weekly.dropna(subset=['consistency_score'])
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šæŒ‰æ—¶æœŸåˆ†ç»„ï¼ˆæ¯5-6ä¸ªèµ›å­£ä¸ºä¸€ç»„ï¼‰
        ax1 = axes[0]
        seasons = sorted(weekly_valid['season'].unique())
        n_groups = 6
        group_size = len(seasons) // n_groups + 1
        
        group_labels = []
        group_data = []
        for i in range(n_groups):
            start_idx = i * group_size
            end_idx = min((i + 1) * group_size, len(seasons))
            if start_idx >= len(seasons):
                break
            group_seasons = seasons[start_idx:end_idx]
            data = weekly_valid[weekly_valid['season'].isin(group_seasons)]['consistency_score'].values
            if len(data) > 0:
                group_data.append(data)
                group_labels.append(f'S{group_seasons[0]}-S{group_seasons[-1]}')
        
        bp = ax1.boxplot(group_data, patch_artist=True, widths=0.6)
        colors = plt.cm.Blues(np.linspace(0.3, 0.8, len(group_data)))
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.8)
        for median in bp['medians']:
            median.set_color('darkred')
            median.set_linewidth(2)
        
        ax1.set_xticklabels(group_labels, fontsize=10)
        ax1.set_xlabel('Season Groups', fontsize=12)
        ax1.set_ylabel('Consistency Score', fontsize=12)
        ax1.set_title('Consistency by Season Period', fontsize=14, fontweight='bold')
        ax1.axhline(y=0.5, linestyle='--', color='red', alpha=0.5, label='Random Guess (0.5)')
        ax1.set_ylim(0, 1.05)
        ax1.legend(loc='lower right')
        ax1.grid(axis='y', alpha=0.3)
        
        # å³å›¾ï¼šæ—¶é—´åºåˆ—è¶‹åŠ¿ï¼ˆæŒ‰èµ›å­£å¹³å‡ï¼‰
        ax2 = axes[1]
        season_stats = weekly_valid.groupby('season')['consistency_score'].agg(['mean', 'std', 'count']).reset_index()
        
        ax2.fill_between(season_stats['season'], 
                        season_stats['mean'] - season_stats['std'],
                        season_stats['mean'] + season_stats['std'],
                        alpha=0.3, color='steelblue', label='Â±1 Std Dev')
        ax2.plot(season_stats['season'], season_stats['mean'], 
                'o-', color='steelblue', linewidth=2, markersize=6, label='Season Mean')
        
        # æ·»åŠ æ»šåŠ¨å¹³å‡çº¿
        if len(season_stats) >= 5:
            rolling_mean = season_stats['mean'].rolling(window=5, center=True).mean()
            ax2.plot(season_stats['season'], rolling_mean, 
                    '--', color='darkred', linewidth=2, label='5-Season Moving Avg')
        
        ax2.axhline(y=0.5, linestyle=':', color='gray', alpha=0.7, label='Random Guess')
        ax2.set_xlabel('Season', fontsize=12)
        ax2.set_ylabel('Consistency Score', fontsize=12)
        ax2.set_title('Consistency Trend Over Seasons', fontsize=14, fontweight='bold')
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc='lower right', fontsize=9)
        ax2.grid(alpha=0.3)
        
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, 'consistency_by_season.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“¦ ç®±çº¿å›¾å·²ä¿å­˜: {output_path}")
    
    def generate_all_visualizations(self) -> None:
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "=" * 60)
        print("  ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (Matplotlib)")
        print("=" * 60)
        
        # 1. Top é€‰æ‰‹ ELO è½¨è¿¹ï¼ˆæŒ‰æ¯”èµ›å‘¨æ•°å¯¹é½ï¼‰
        self.plot_elo_trajectories(top_n=12)
        
        # 2. æœ€è¿‘3ä¸ªèµ›å­£çš„å•ç‹¬ ELO è½¨è¿¹å›¾
        seasons = sorted(self.results['season'].unique())[-3:]
        for s in seasons:
            self.plot_elo_by_season(s, top_n=8)
        
        # 3. æœ€è¿‘3ä¸ªèµ›å­£çš„çƒ­åŠ›å›¾
        for s in seasons:
            self.plot_fan_vote_heatmap(s)
        
        # 4. æ¨¡å‹è¯Šæ–­
        self.plot_model_diagnostics()
        
        # 5. èµ›å­£å¯¹æ¯”é›·è¾¾
        self.plot_season_comparison_radar()
        
        # 6. æœ€ç»ˆæ’å
        self.plot_final_rankings(top_n=25)
        
        # 7. ELO åˆ†å¸ƒ
        self.plot_elo_distribution()
        
        # 8. å„èµ›å­£ä¸€è‡´æ€§ç®±çº¿å›¾
        self.plot_consistency_by_season()
        
        # 9. ç¤ºä¾‹æ·˜æ±°æ¦‚ç‡å›¾
        if self.estimator.weekly_distributions:
            last_dist = self.estimator.weekly_distributions[-10]
            self.plot_elimination_probability(last_dist['season'], last_dist['week'])
        
        print(f"\nâœ… æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜è‡³ '{self.output_dir}' ç›®å½•")


# ============== ä¸»ç¨‹åºå…¥å£ ==============

def main():
    """ä¸»è¿è¡Œç¨‹åº"""
    import time
    
    input_csv = 'cleaned_weekly_data.csv'
    
    if not os.path.exists(input_csv):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {input_csv}")
        return
    
    print("=" * 70)
    print("  è´å¶æ–¯ ELO + é«˜çº§è’™ç‰¹å¡æ´›ç²‰ä¸æŠ•ç¥¨é€†å‘æ¨ç®—ç³»ç»Ÿ v2.0")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    df_cleaned = pd.read_csv(input_csv)
    print(f"   æ•°æ®è§„æ¨¡: {len(df_cleaned):,} è¡Œ")
    print(f"   èµ›å­£æ•°é‡: {df_cleaned['season'].nunique()}")
    print(f"   é€‰æ‰‹æ•°é‡: {df_cleaned['name'].nunique()}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    estimator = BayesianEloEstimator(
        base_k_factor=48.0,
        temperature=150.0,
        n_simulations=3000,
        noise_std=0.3,
        judge_weight=0.5,
        rd_decay=0.95,
        use_adaptive_params=True,
        memory_decay=0.92
    )
    
    backend = 'Numba JIT åŠ é€Ÿ' if NUMBA_AVAILABLE else 'çº¯ NumPy'
    print(f"\nâš¡ è®¡ç®—åç«¯: {backend}")
    print(f"ğŸ“ ç®—æ³•ç‰¹æ€§: è´å¶æ–¯æ›´æ–° + åˆ†å±‚è’™ç‰¹å¡æ´› + Glicko-2 é£æ ¼ RD")
    
    # è¿è¡Œæ¨æ–­
    print("\nğŸ”„ å¼€å§‹é€†å‘æ¨ç®—ç²‰ä¸æŠ•ç¥¨...")
    start_time = time.perf_counter()
    
    final_results = estimator.run_inference(df_cleaned)
    
    elapsed = time.perf_counter() - start_time
    
    # ä¿å­˜ç»“æœ
    output_file = 'QFâ€˜s solution/Q2_bayes/Q2_1/percent_fan_vote_estimates_weekly.csv'
    final_results.to_csv(output_file, index=False, float_format='%.6f')
    
    # è¾“å‡ºç»Ÿè®¡
    stats = estimator.get_statistics()
    print(f"\nâœ… æ¨æ–­å®Œæˆ!")
    print(f"   è€—æ—¶: {elapsed:.2f} ç§’")
    print(f"   æ€»æ¨¡æ‹Ÿæ¬¡æ•°: {stats['total_simulations']:,}")
    print(f"   é€‰æ‰‹æ€»æ•°: {stats['total_contestants']}")
    print(f"   å¹³å‡ ELO: {stats['avg_elo']:.1f} Â± {stats['elo_std']:.1f}")
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # æ¨¡å‹è¯„ä¼°
    print("\n" + "=" * 70)
    print("  æ¨¡å‹è¯„ä¼°æŒ‡æ ‡")
    print("=" * 70)
    weekly = final_results.groupby(['season', 'week']).first()
    print(f"   å¹³å‡ä¸€è‡´æ€§åˆ†æ•°: {weekly['consistency_score'].mean():.4f}")
    print(f"   å¹³å‡ç¡®å®šæ€§åˆ†æ•°: {weekly['certainty_score'].mean():.4f}")
    print(f"   å¹³å‡ KL æ•£åº¦: {weekly['kl_divergence'].mean():.4f}")
    print(f"   ä¸€è‡´æ€§ > 0.5 çš„å‘¨æ•°æ¯”ä¾‹: {(weekly['consistency_score'] > 0.5).mean():.2%}")
    
    # Top 10 æ’å
    print("\n" + "=" * 70)
    print("  Top 10 é€‰æ‰‹ (æœ€ç»ˆ ELO æ’å)")
    print("=" * 70)
    rankings = estimator.get_final_rankings()
    print(rankings.head(10).to_string(index=False))
    
    # ç”Ÿæˆå¯è§†åŒ–
    # visualizer = EloVisualizer(estimator, final_results)
    # visualizer.generate_all_visualizations()


if __name__ == "__main__":
    main()