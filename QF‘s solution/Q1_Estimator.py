# evaluator_with_metrics.py

import json
import os
from collections import defaultdict
import numpy as np

# âœ… ä»å·²æœ‰æ¨¡å—å¯¼å…¥åŸºç¡€ç±»ï¼ˆå‡è®¾ Q1_Basic_Model.py åœ¨åŒä¸€ç›®å½•ï¼‰
from Q1_Basic_Model import InverseFanVoteEstimator


# ==========================================
# æ‰©å±•è¯„ä¼°å™¨ï¼šåœ¨åŸæ¨¡å‹åŸºç¡€ä¸Šå¢åŠ ç¨³å®šæ€§æŒ‡æ ‡è®¡ç®—
# ==========================================
class Evaluator(InverseFanVoteEstimator):
    """
    Extends the base estimator to run multiple global simulations
    and calculate Consistency (probability assigned to actual loser)
    and Certainty (standard deviation across runs).
    """

    def run_with_metrics(self, n_global_runs=20):
        print(f"Running stability analysis with {n_global_runs} global simulations...")
        
        all_runs_history = defaultdict(lambda: defaultdict(list))
        consistency_scores = defaultdict(list)
        
        for run_id in range(n_global_runs):
            if run_id % 5 == 0:
                print(f"  - Simulation run {run_id}/{n_global_runs}")

            # é‡ç½® X åˆ°å‡åŒ€å…ˆéªŒï¼ˆå…³é”®ï¼ä¿è¯æ¯æ¬¡æ¨¡æ‹Ÿç‹¬ç«‹ï¼‰
            self.X = np.ones(len(self.contestants)) / len(self.contestants)
            
            # æ¨¡æ‹Ÿæ¯å‘¨ï¼ˆæ ¹æ®æ•°æ®è‡ªåŠ¨æ¨æ–­æœ€å¤§å‘¨æ•°ï¼‰
            for week in range(1, self.max_data_week + 1):
                payload = self.get_week_data_and_target(week)  # â† æ¥è‡ªåŸç±»çš„æ–¹æ³•
                if not payload:
                    break
                names, judges, elim_idx, global_idx = payload
                
                # å†…å±‚ä¼˜åŒ–å¾ªç¯
                for _ in range(50):
                    current_priors = self.normalize(self.X[global_idx])
                    probs = self.simulate_step(judges, current_priors)
                    self.update_parameters(global_idx, probs, elim_idx)
                
                # è®¡ç®—ä¸€è‡´æ€§ï¼šæ¨¡å‹å¯¹å®é™…æ·˜æ±°è€…çš„é¢„æµ‹æ¦‚ç‡
                final_probs = self.simulate_step(judges, self.normalize(self.X[global_idx]))
                if len(elim_idx) > 0:
                    actual_loser_prob = final_probs[elim_idx[0]]
                    consistency_scores[week].append(actual_loser_prob)
                else:
                    consistency_scores[week].append(1.0)  # æ— æ·˜æ±°è§†ä¸ºå®Œç¾ä¸€è‡´
                
                # è®°å½•æœ¬è½®ä¼°è®¡å€¼ï¼ˆç”¨äºç¡®å®šæ€§è®¡ç®—ï¼‰
                current_priors = self.normalize(self.X[global_idx])
                for name, score in zip(names, current_priors):
                    all_runs_history[week][name].append(score)
                
                # æ—¶é—´æ¼”åŒ–ï¼ˆè®°å¿†æœºåˆ¶ï¼‰
                judge_share = self.normalize(judges)
                new_priors = self.alpha * current_priors + (1 - self.alpha) * judge_share
                self.X[global_idx] = new_priors
                self.X = self.normalize(self.X)

        return self._compile_report(all_runs_history, consistency_scores)

    def _compile_report(self, history, consistency):
        report = []
        for week in sorted(history.keys()):
            week_data = history[week]
            avg_consistency = float(np.mean(consistency[week]))
            
            contestants_stats = {}
            for name, scores in week_data.items():
                scores = np.array(scores)
                mean_score = float(np.mean(scores))
                std_dev = float(np.std(scores))
                cv = float(std_dev / mean_score) if mean_score > 0 else 0.0
                contestants_stats[name] = {
                    'mean_vote': mean_score,
                    'certainty_score': std_dev,
                    'cv': cv
                }
            
            report.append({
                'week': week,
                'consistency_index': avg_consistency,
                'contestants': contestants_stats
            })
        return report


# ==========================================
# å°è£…å‡½æ•°ï¼šæ¥å—è·¯å¾„å‚æ•°ï¼Œä¿å­˜ JSON
# ==========================================
def evaluate_and_save(input_csv_path: str, output_json_path: str, season_id: int = 5, n_global_runs: int = 20):
    """
    è¿è¡Œå¤šè½®è¯„ä¼°å¹¶ä¿å­˜ç»“æœã€‚
    
    Parameters:
        input_csv_path: è¾“å…¥ CSV è·¯å¾„
        output_json_path: è¾“å‡º JSON è·¯å¾„
        season_id: èµ›å­£ ID
        n_global_runs: æ¨¡æ‹Ÿæ¬¡æ•°
    """
    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
    
    evaluator = Evaluator(
        data_path=input_csv_path,
        season_id=season_id,
        n_trials=1000,
        step_size=0.05,
        memory_alpha=0.7
    )
    
    metrics = evaluator.run_with_metrics(n_global_runs=n_global_runs)
    
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Results saved to: {output_json_path}")
    return metrics


# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # ğŸ”§ è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ä»¥ä¸‹ä¸¤è¡Œ
    INPUT_PATH = '/Users/liuqiufan/Documents/SJTU_Local/MCM2026/QFâ€˜s solution/2026_MCM_Problem_C/2026_MCM_Problem_C_Data.csv'
    OUTPUT_PATH = '/Users/liuqiufan/Documents/SJTU_Local/MCM2026/QFâ€˜s solution/2026_MCM_Problem_C/evaluation_metrics.json'

    results = evaluate_and_save(
        input_csv_path=INPUT_PATH,
        output_json_path=OUTPUT_PATH,
        season_id=5,
        n_global_runs=20
    )

    # é¢„è§ˆå‰ä¸¤å‘¨
    print("\nğŸ” Preview of first two weeks:")
    print(json.dumps(results[:2], indent=2))